{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will implement a sample neural network for audio processing. You will learn how to load data, build data loaders and networks, and how to train and evaluate a network. Learning both the fundamental concepts and advanced applications of neural networks become necessary and rewarding nowadays.\n",
    "\n",
    "You can easily find tremendous amount of tutorials on neural networks - both the platforms/libraries and the basic concepts. You can also easily find a lot of opensource projects for all the fancy models proposed and published in recent years. Here we will focus on a simple pipeline for a speech processing project by building an autoencoder for a given input signal. I hope that by going through this pipeline you can get a clear picture of what you need to do in training and evaluation of a neural network.\n",
    "\n",
    "In general a machine learning project consists of 3 stages: **data preparation**, **model definition**, and **training and evaluation functions**. We will go through each of them. Here we use [**Pytorch**](https://pytorch.org/) for all the models since Pytorch is one of the most widely-used deep learning platforms nowadays and is especially easy to learn for beginners.\n",
    "\n",
    "\n",
    "Note that in this course we **do not** assume that you have a GPU for model training - the data size and model size are selected so that you can finish all the homeworks even with your laptop's CPU. So don't worry if your laptop is not that powerful!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we first import all the libraries we intend to use\n",
    "import numpy as np\n",
    "import librosa\n",
    "import os\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I will show my way to do data preparation and dataset generation, but there are definitely many other ways to do that \n",
    "\n",
    "I'll start with the raw waveforms - suppose we have a folder that contains many waveforms, and we want to load them, process them and save them into a dataset for easy loading during training/testing. The audio I/O is already covered in the tutorial for signal processing with Python, and here I won't describe that again.\n",
    "\n",
    "There are several libraries that I use the most. Despite for the standard libraries and audio I/O library *librosa* and *soundfile*, the library I typically use to generate (small-scale) dataset is [*h5py*](https://www.h5py.org/), a Python wrapper for the *hdf5* format files. HDF is the abbreviation of Hierarchical Data Format, a binary data format for storing datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the path of a directory, we can scan through it, find all the .wav files, and save their paths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of .wav files: 319\n",
      "Example path: ../wav_data/sx449.wav\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "dir_path = '../wav_data'  # directory path\n",
    "\n",
    "# walk through the directory, find the files with .wav extension\n",
    "wav_files = []\n",
    "for (dirpath, dirnames, filenames) in os.walk(dir_path):\n",
    "    for file in filenames:\n",
    "        if '.wav' in file:\n",
    "            wav_files.append(dirpath+'/'+file)\n",
    "        \n",
    "num_data = len(wav_files)\n",
    "print('Number of .wav files: {:2d}'.format(num_data))\n",
    "print('Example path: ' + wav_files[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we simply use 319 utterances as an example. Once we have the path to the waveforms, we can load each of them, calculate any feature we want (e.g. spectrogram), and save them to a dataset. Here we will save the raw waveforms as well as the log-power spectrograms.\n",
    "\n",
    "Note that *hdf5* format assumes that each item in the dataset has a fixed shape. For 1-D feature vectors, all features should have the same length, and for 2-D feature matrices all features should have both the same width and height. For waveforms with different length, we need to either truncate them or zero-pad them to the same length. For simplicity here we set the maximum length to 2 seconds, and truncate or pad them accordingly.\n",
    "\n",
    "We split the 319 files into a training set, a validation set and a test set. I'll use 200 for training, 50 for validation and 69 for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create blank datasets with h5py\n",
    "\n",
    "tr_name = 'tr_set.hdf5'\n",
    "val_name = 'val_set.hdf5'\n",
    "test_name = 'test_set.hdf5'\n",
    "\n",
    "tr_dataset = h5py.File(tr_name, 'a')\n",
    "val_dataset = h5py.File(val_name, 'a')\n",
    "test_dataset = h5py.File(test_name, 'a')\n",
    "\n",
    "# maximum length of waveforms \n",
    "sr = 16000  # sample rate\n",
    "length_wave_max = 2 * sr\n",
    "\n",
    "# STFT window and hop size\n",
    "n_fft = 512\n",
    "n_hop = n_fft // 4\n",
    "\n",
    "for i in range(num_data):\n",
    "    # first 200 files for training, next 50 for validation, and last 69 for testing\n",
    "    \n",
    "    # load the wavefiles\n",
    "    y ,_ = librosa.load(wav_files[i],sr=sr)  # the default sample rate for them is 16kHz, but you can also change that\n",
    "    \n",
    "    # truncate or zero-pad the signal\n",
    "    y = y[:length_wave_max]\n",
    "    if len(y) < length_wave_max:\n",
    "        y = np.concatenate([y, np.zeros(length_wave_max-len(y))])\n",
    "    \n",
    "    # calculate log-power spectrogram in decibel scale\n",
    "    spec = librosa.stft(y, n_fft=n_fft, hop_length=n_hop)\n",
    "    # the 1e-8 here is added for numerical stability, in case that log10(0) happens\n",
    "    log_power_spec = 10*np.log10(np.abs(spec)**2 + 1e-8)  # shape: (n_fft/2+1, T)\n",
    "    num_frame = log_power_spec.shape[1]  # this should be the same for all the utterances, since we map them into same length\n",
    "    \n",
    "    # save them into the hdf5 file\n",
    "    \n",
    "    if i < 200:\n",
    "        if i == 0:\n",
    "            # create sub-datasets\n",
    "            tr_dataset.create_dataset('waveform', shape=(200, length_wave_max), dtype=np.float32)\n",
    "            tr_dataset.create_dataset('spec', shape=(200, n_fft//2+1, num_frame), dtype=np.float32)\n",
    "        \n",
    "        tr_dataset['waveform'][i] = y\n",
    "        tr_dataset['spec'][i] = log_power_spec\n",
    "    elif i < 250:\n",
    "        if i == 200:\n",
    "            # create sub-datasets\n",
    "            val_dataset.create_dataset('waveform', shape=(50, length_wave_max), dtype=np.float32)\n",
    "            val_dataset.create_dataset('spec', shape=(50, n_fft//2+1, num_frame), dtype=np.float32)\n",
    "        \n",
    "        val_dataset['waveform'][i-200] = y\n",
    "        val_dataset['spec'][i-200] = log_power_spec\n",
    "    else:\n",
    "        if i == 250:\n",
    "            # create sub-datasets\n",
    "            test_dataset.create_dataset('waveform', shape=(69, length_wave_max), dtype=np.float32)\n",
    "            test_dataset.create_dataset('spec', shape=(69, n_fft//2+1, num_frame), dtype=np.float32)\n",
    "        \n",
    "        test_dataset['waveform'][i-250] = y\n",
    "        test_dataset['spec'][i-250] = log_power_spec\n",
    "    \n",
    "tr_dataset.close()\n",
    "val_dataset.close()\n",
    "test_dataset.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In most of the cases we need to do some normalization or whitening on the training set. Here we normalize the log-power spectrogram across the frames, so that each frame in the entire dataset should be zero mean and unit variance. This is often called the mean-variance normalization (MVN) operation. Note that we calculate the mean and variance statistics only using the training set, and apply them to both validation set and testing set.\n",
    "\n",
    "In certain cases the normalization is done within the neural networks. We will probably get into it in future homeworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the training set\n",
    "tr_dataset = h5py.File(tr_name, 'a')\n",
    "tr_spec = tr_dataset['spec']  # shape: (num_data, n_fft/2+1, T)\n",
    "tr_spec = np.transpose(tr_spec[:], (0, 2, 1)).reshape(-1, n_fft//2+1)  # shape: (num_data*T, n_fft/2+1)\n",
    "tr_mean = np.mean(tr_spec, axis=0)  # shape: (n_fft/2+1)\n",
    "tr_var = np.var(tr_spec, axis=0)  # shape: (n_fft/2+1)\n",
    "tr_std = np.sqrt(tr_var + 1e-8)  # again for numerical stability\n",
    "\n",
    "# apply normalization to all the datasets\n",
    "\n",
    "val_dataset = h5py.File(val_name, 'a')\n",
    "test_dataset = h5py.File(test_name, 'a')\n",
    "\n",
    "tr_dataset['spec'][:] = (tr_dataset['spec'][:] - tr_mean.reshape(1,-1,1)) / tr_std.reshape(1,-1,1)\n",
    "val_dataset['spec'][:] = (val_dataset['spec'][:] - tr_mean.reshape(1,-1,1)) / tr_std.reshape(1,-1,1)\n",
    "test_dataset['spec'][:] = (test_dataset['spec'][:] - tr_mean.reshape(1,-1,1)) / tr_std.reshape(1,-1,1)\n",
    "\n",
    "tr_dataset.close()\n",
    "val_dataset.close()\n",
    "test_dataset.close()\n",
    "\n",
    "# save the mean and std information in files\n",
    "np.save('training_mean', tr_mean)\n",
    "np.save('training_std', tr_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the datasets generated and ready for use. In Pytorch, we can define *DataLoader* classes to load our saved datasets into Pytorch-friendly formats that can be used by the models we will define in the next session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "# a class to load the saved h5py dataset\n",
    "class dataset_pipeline(Dataset):\n",
    "    def __init__(self, path):\n",
    "        super(dataset_pipeline, self).__init__()\n",
    "\n",
    "        self.h5pyLoader = h5py.File(path, 'r')\n",
    "        \n",
    "        self.spec = self.h5pyLoader['spec']\n",
    "        \n",
    "        self._len = self.spec.shape[0]  # number of utterances\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        spec_item = torch.from_numpy(self.spec[index].astype(np.float32))\n",
    "            \n",
    "        return spec_item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self._len\n",
    "    \n",
    "# define data loaders\n",
    "train_loader = DataLoader(dataset_pipeline('tr_set.hdf5'), \n",
    "                          batch_size=batch_size, \n",
    "                          shuffle=True,  # this ensures that the sequential order of the training samples will be shuffled for different training epochs\n",
    "                         )\n",
    "\n",
    "validation_loader = DataLoader(dataset_pipeline('val_set.hdf5'), \n",
    "                               batch_size=batch_size, \n",
    "                               shuffle=False,  # typically we fix the sequential order of the validation samples\n",
    "                              )\n",
    "\n",
    "dataset_len = len(train_loader)\n",
    "log_step = dataset_len // 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will go through the implementation of 3 types of basic layers here - fully-connected layer (FC layer), recurrent layer, and convolutional layer. After that we will show how to construct a deeper network, and you will be asked to implement one network by yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 FC Layers\n",
    "\n",
    "An FC layer simply contains a weight matrix together with a bias vector, and performs a linear transformation on an input feature followed by a nonlinear activation function. *help(nn.Linear)* provides a detailed description about the weights and the operation of an FC layer. (always use *help()* when you don't know what happens inside a function!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn((10, 100))  # create a random input features of shape (batch_size, feature_dimension)\n",
    "FC_layer = nn.Linear(100, 20)  # An FC layer\n",
    "FC_activation = nn.ReLU()  # nonlinear activation function\n",
    "output = FC_activation(FC_layer(input))  # process the input\n",
    "print(\"Shape of input:\", tuple(input.data.shape))\n",
    "print(\"Shape of output:\", tuple(output.data.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Recurrent Layers\n",
    "\n",
    "Recurrent layers are used for sequence modeling. Time steps in the input sequence is sent to the recurrent layer in a sequential order. A basic recurrent layer not only contains a weight matrix and a bias vector, but also maintains a * hidden state* that store the information processed so far. In other words, the output at the current time step not only depends on the current input, but also depends on the historical input at all previous steps (summarized in the current hidden state)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn((2, 10, 100))  # create a batch of random sequential features with shape (batch_size, sequence_length, feature_dimension)\n",
    "RNN_layer = nn.RNN(100, 20, num_layers=1, nonlinearity='tanh', batch_first=True)  # A basic RNN layer\n",
    "init_hidden = torch.zeros(1, 2, 20)  # initial hidden state is typically zero matrices\n",
    "output, output_hidden = RNN_layer(input, init_hidden)  # process the input\n",
    "# you can also set init_hidden to None, and the layer with automatically treat it as zero matrices.\n",
    "#output, output_hidden = RNN_layer(input, None)  # the output will be the same as above\n",
    "\n",
    "print(\"Shape of input:\", tuple(input.data.shape))\n",
    "print(\"Shape of output:\", tuple(output.data.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beyond the basic RNN layer, we typically use more advanced RNN layers with a memory cell. One of the most famous layer is the long-short term memory (LSTM) layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_layer = nn.LSTM(100, 20, num_layers=1, batch_first=True)  # A basic RNN layer\n",
    "init_hidden = torch.zeros(1, 2, 20)  # initial hidden state is typically zero matrices\n",
    "init_cell = torch.zeros(1, 2, 20)  # now you also have a initial cell state\n",
    "output, (output_hidden, output_cell) = LSTM_layer(input, (init_hidden, init_cell))  # process the input\n",
    "# similarly, init_hidden and init_cell can both be None\n",
    "# output, (output_hidden, output_cell) = LSTM_layer(input, (None, None))  # same as above\n",
    "print(\"Shape of input:\", tuple(input.data.shape))\n",
    "print(\"Shape of output:\", tuple(output.data.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Convolutional Layers\n",
    "\n",
    "Convolutional layers are the most popular building blocks for computer vision tasks, but we can also use them in speech processing networks. In image processing people typically use 2-D convolutional blocks as images are 2-D, but in audio processing we can either use 1-D convolution or 2-D convolution, depending on our input feature.\n",
    "\n",
    "If we want to directly manipulate the input waveforms, we can use 1-D convolutional layers (remember that Fourier transform can also be defined by the convolution between the signal and the sinusoid basis functions!). If we want to use spectrograms as input feature, we can use either 1-D or 2-D convolutional layers, where 1-D convolution treats each frame as an entire feature and performs sequence modeling, and 2-D convolution treats the spectrogram as an image.\n",
    "\n",
    "Kernel size, stride size and padding size are extremely important in convolutional layers. In speech and audio processing you often want to have the same output size as the input, and in this case you need to be very careful about the stride size and padding size as they need to be manually calculated from your kernel size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn((2, 1, 160))  # create a batch of random input waveforms with shape (batch_size, in_channels, num_samples)\n",
    "Conv1d_layer = nn.Conv1d(in_channels=1, out_channels=16, kernel_size=3, padding=0, stride=1)  # A basic Conv1d layer\n",
    "Conv1d_activation = nn.ReLU()  # nonlinera activation\n",
    "output = Conv1d_activation(Conv1d_layer(input))  # process the input\n",
    "\n",
    "print(\"Shape of input:\", tuple(input.data.shape))\n",
    "print(\"Shape of output:\", tuple(output.data.shape))\n",
    "\n",
    "# play with different configurations of padding size and stride size\n",
    "Conv1d_layer = nn.Conv1d(in_channels=1, out_channels=16, kernel_size=3, padding=1, stride=1)\n",
    "output = Conv1d_activation(Conv1d_layer(input))  # process the input\n",
    "\n",
    "print(\"Shape of input:\", tuple(input.data.shape))\n",
    "print(\"Shape of output:\", tuple(output.data.shape))\n",
    "\n",
    "Conv1d_layer = nn.Conv1d(in_channels=1, out_channels=16, kernel_size=3, padding=1, stride=2)\n",
    "output = Conv1d_activation(Conv1d_layer(input))  # process the input\n",
    "\n",
    "print(\"Shape of input:\", tuple(input.data.shape))\n",
    "print(\"Shape of output:\", tuple(output.data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn((2, 1, 33, 40))  # create a batch of random input spectrograms with shape (batch_size, in_channels, frequency_dim, time_step)\n",
    "Conv2d_layer = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=(3, 3), padding=(1, 1), stride=(1, 1))  # A basic Conv1d layer\n",
    "Conv2d_activation = nn.ReLU()  # nonlinera activation\n",
    "output = Conv2d_activation(Conv2d_layer(input))  # process the input\n",
    "\n",
    "print(\"Shape of input:\", tuple(input.data.shape))\n",
    "print(\"Shape of output:\", tuple(output.data.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct a Deeper Model\n",
    "\n",
    "Now we know how to build single FC/RNN/CNN layers. It's time for us to build a deeper network using those basic layers. Let's start with a multilayer perceptron (MLP) built from FC layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a 3 layer MLP\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, feature_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        # layers, and activation function\n",
    "        \n",
    "        self.feature_dim = feature_dim\n",
    "        self.hidden_unit = 128  # number of hidden units\n",
    "        \n",
    "        # input layer\n",
    "        # nn.Sequential allows you to wrap multiple layers into a single large layer\n",
    "        # all the layers inside an nn.Sequential object will run in their sequential order\n",
    "        self.layer1 = nn.Sequential(nn.Linear(self.feature_dim, self.hidden_unit),\n",
    "                                    nn.Sigmoid()\n",
    "                                   )\n",
    "        \n",
    "        # do the same for two other layers\n",
    "        # hidden layer\n",
    "        self.layer2 = nn.Sequential(nn.Linear(self.hidden_unit, self.hidden_unit),\n",
    "                                    nn.Sigmoid()\n",
    "                                   )\n",
    "        \n",
    "        # output layer\n",
    "        # here we do not apply a nonlinear activation function as we want to do autoencoding\n",
    "        self.layer3 = nn.Linear(self.hidden_unit, self.feature_dim)\n",
    "        \n",
    "    # the function for the forward pass of network (i.e. from input to output)\n",
    "    def forward(self, input):\n",
    "        # the input is a batch of spectrograms with shape (batch_size, frequency_dim, time_step)\n",
    "        # we want the MLP to be applied to the frequency_dim dimension, i.e. process each frame independently\n",
    "        # we need to rotate the last two dimensions and reshape it into a 2-D matrix with shape (batch_size*time_step, frequency_dim)\n",
    "        \n",
    "        batch_size, freq_dim, time_step = input.shape\n",
    "        \n",
    "        input = input.transpose(1, 2).contiguous()  # (batch, time, freq), swap the two dimensions\n",
    "        input = input.view(batch_size*time_step, freq_dim)  # (batch*time, freq), reshaping\n",
    "        \n",
    "        # pass it through the three layers\n",
    "        output = self.layer1(input)  # (batch*time, hidden_unit)\n",
    "        output = self.layer2(output)  # (batch*time, hidden_unit)\n",
    "        output = self.layer3(output)  # (batch*time, freq)\n",
    "        \n",
    "        # reshape back\n",
    "        output = output.view(batch_size, time_step, freq_dim)  # (batch, time, freq)\n",
    "        output = output.transpose(1, 2).contiguous()  # (batch, freq, time), swap the two dimension back\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test if the model works well\n",
    "model_MLP = MLP(feature_dim=257)\n",
    "\n",
    "input = torch.randn((2, 257, 40))\n",
    "output = model_MLP(input)\n",
    "\n",
    "print(\"Shape of input:\", tuple(input.data.shape))\n",
    "print(\"Shape of output:\", tuple(output.data.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective and Optimizer\n",
    "\n",
    "Now we have the model and the data prepared. We need to define an optimizer that allows us the train the model, and an objective function that the optimizer uses to measure whether the model is generating correct outputs and update the model weights based on the values of the objective functions. This is done by backpropagation - I believe you all know this.\n",
    "\n",
    "A *learning rate* needs to be defined for an optimizer - it determines how much the model parameters are adjusted each time it receives a new batch of training data. There are many discussions on the selection of learning rates - large learning rates can prevent the successful convergence of the model, and small learning rates can make the convergence speed way too slow. Here we typically set the initial learning rate to 0.01 in an empiricaly way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the optimizer\n",
    "# typically I use Adam, but you can use others\n",
    "optimizer = optim.Adam(model_MLP.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define the objective function. One of the most widely-used objective function is the mean-square error (MSE) function that compares the Euclidean distance between the input and output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(input, output):\n",
    "    # input shape: (batch, freq, time)\n",
    "    # output shape: (batch, freq, time)\n",
    "    \n",
    "    batch_size = input.shape[0]\n",
    "    input = input.view(batch_size, -1)\n",
    "    output = output.view(batch_size, -1)\n",
    "    loss = (input - output).pow(2).mean(1)  # (batch_size, 1)\n",
    "    return loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing\n",
    "\n",
    "Finally we can define the functions for training and validation. For the task of autoencoding, the input and the output of the model are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for training and validation\n",
    "\n",
    "def train(model, epoch, versatile=True):\n",
    "    start_time = time.time()\n",
    "    model = model.train()  # set the model to training mode. Always do this before you start training!\n",
    "    train_loss = 0.\n",
    "    \n",
    "    # load batch data\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        batch_spec = data\n",
    "        \n",
    "        # clean up the gradients in the optimizer\n",
    "        # this should be called for each batch\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(batch_spec)\n",
    "        \n",
    "        # MSE as objective\n",
    "        loss = MSE(batch_spec, output)\n",
    "        \n",
    "        # automatically calculate the backward pass\n",
    "        loss.backward()\n",
    "        # perform the actual backpropagation\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.data.item()\n",
    "        \n",
    "        # OPTIONAL: you can print the training progress \n",
    "        if versatile:\n",
    "            if (batch_idx+1) % log_step == 0:\n",
    "                elapsed = time.time() - start_time\n",
    "                print('| epoch {:3d} | {:5d}/{:5d} batches | ms/batch {:5.2f} | MSE {:5.4f} |'.format(\n",
    "                    epoch, batch_idx+1, len(train_loader),\n",
    "                    elapsed * 1000 / (batch_idx+1), \n",
    "                    train_loss / (batch_idx+1)\n",
    "                    ))\n",
    "    \n",
    "    train_loss /= (batch_idx+1)\n",
    "    print('-' * 99)\n",
    "    print('    | end of training epoch {:3d} | time: {:5.2f}s | MSE {:5.4f} |'.format(\n",
    "            epoch, (time.time() - start_time), train_loss))\n",
    "    \n",
    "    return train_loss\n",
    "        \n",
    "def validate(model, epoch):\n",
    "    start_time = time.time()\n",
    "    model = model.eval()  # set the model to evaluation mode. Always do this during validation or test phase!\n",
    "    validation_loss = 0.\n",
    "    \n",
    "    # load batch data\n",
    "    for batch_idx, data in enumerate(validation_loader):\n",
    "        batch_spec = data\n",
    "        \n",
    "        # you don't need to calculate the backward pass and the gradients during validation\n",
    "        # so you can call torch.no_grad() to only calculate the forward pass to save time and memory\n",
    "        with torch.no_grad():\n",
    "        \n",
    "            output = model(batch_spec)\n",
    "        \n",
    "            # MSE as objective\n",
    "            loss = MSE(batch_spec, output)\n",
    "        \n",
    "            validation_loss += loss.data.item()\n",
    "    \n",
    "    validation_loss /= (batch_idx+1)\n",
    "    print('    | end of validation epoch {:3d} | time: {:5.2f}s | MSE {:5.4f} |'.format(\n",
    "            epoch, (time.time() - start_time), validation_loss))\n",
    "    print('-' * 99)\n",
    "    \n",
    "    return validation_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define some hyperparameters and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "\n",
    "total_epoch = 100  # train the model for 100 epochs\n",
    "model_save = 'best_model_MLP.pt'  # path to save the best validation model\n",
    "\n",
    "# main function\n",
    "\n",
    "training_loss = []\n",
    "validation_loss = []\n",
    "\n",
    "for epoch in range(1, total_epoch + 1):\n",
    "    training_loss.append(train(model_MLP, epoch))\n",
    "    validation_loss.append(validate(model_MLP, epoch))\n",
    "    \n",
    "    if training_loss[-1] == np.min(training_loss):\n",
    "        print('      Best training model found.')\n",
    "    if validation_loss[-1] == np.min(validation_loss):\n",
    "        # save current best model on validation set\n",
    "        with open(model_save, 'wb') as f:\n",
    "            torch.save(model_MLP.state_dict(), f)\n",
    "            print('      Best validation model found and saved.')\n",
    "    \n",
    "    print('-' * 99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test phase is the same as the validation phase, and I skip the details here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "It's your turn to play with the models - implement a 2-layer LSTM network. The template is provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a 2-layer LSTM\n",
    "class DeepLSTM(nn.Module):\n",
    "    def __init__(self, feature_dim):\n",
    "        super(DeepLSTM, self).__init__()\n",
    "        \n",
    "        # layers, and activation function\n",
    "        \n",
    "        self.feature_dim = feature_dim\n",
    "        self.hidden_unit = 128  # number of hidden units\n",
    "        \n",
    "        # TODO: first LSTM layer\n",
    "        self.LSTM1 = nn.LSTM(self.feature_dim, self.hidden_unit, num_layers=1, batch_first=True)\n",
    "        \n",
    "        # TODO: second LSTM layer\n",
    "        self.LSTM2 = nn.LSTM(self.hidden_unit, self.hidden_unit, num_layers=1, batch_first=True)\n",
    "        \n",
    "        # you can also jointly define a 2-layer LSTM \n",
    "        # self.deep_LSTM = nn.LSTM(self.feature_dim, self.hidden_unit, num_layers=2, batch_first=True)\n",
    "        \n",
    "        # output layer\n",
    "        # this is an FC layer \n",
    "        self.output = nn.Linear(self.hidden_unit, self.feature_dim)\n",
    "        \n",
    "    # the function for the forward pass of network (i.e. from input to output)\n",
    "    def forward(self, input):\n",
    "        # the input is a batch of spectrograms with shape (batch_size, frequency_dim, time_step)\n",
    "        \n",
    "        batch_size, freq_dim, time_step = input.shape\n",
    "        \n",
    "        # note that LSTM layers require input with shape (batch_size, sequence_length, feature_dim)\n",
    "        # so here we need to reshape the input\n",
    "        \n",
    "        input = input.transpose(1, 2).contiguous()  # (batch, time, freq), swap the two dimensions\n",
    "        \n",
    "        # TODO: pass it through the 2 LSTM layers\n",
    "        \n",
    "        output, output_hidden = self.LSTM1(input, None)\n",
    "        output, output_hidden = self.LSTM2(output, None)\n",
    "        \n",
    "        # the output should have shape (batch, time, hidden_unit)\n",
    "        # pass to the output layer\n",
    "        \n",
    "        output = self.output(output.contiguous().view(batch_size*time_step, -1))  # *batch*time, freq\n",
    "        \n",
    "        # reshape back\n",
    "        output = output.view(batch_size, time_step, freq_dim)  # (batch, time, freq)\n",
    "        output = output.transpose(1, 2).contiguous()  # (batch, freq, time), swap the two dimension back\n",
    "        \n",
    "        return output\n",
    "    \n",
    "model_LSTM = DeepLSTM(feature_dim=257)\n",
    "optimizer = optim.Adam(model_LSTM.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train the model you implemented\n",
    "\n",
    "total_epoch = 100  # train the model for 100 epochs\n",
    "model_save = 'best_model_LSTM.pt'  # path to save the best validation model\n",
    "\n",
    "# main function\n",
    "\n",
    "training_loss = []\n",
    "validation_loss = []\n",
    "\n",
    "for epoch in range(1, total_epoch + 1):\n",
    "    training_loss.append(train(model_LSTM, epoch))\n",
    "    validation_loss.append(validate(model_LSTM, epoch))\n",
    "    \n",
    "    if training_loss[-1] == np.min(training_loss):\n",
    "        print('      Best training model found.')\n",
    "    if validation_loss[-1] == np.min(validation_loss):\n",
    "        # save current best model on validation set\n",
    "        with open(model_save, 'wb') as f:\n",
    "            torch.save(model_LSTM.state_dict(), f)\n",
    "            print('      Best validation model found and saved.')\n",
    "    \n",
    "    print('-' * 99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the training speed and training and validation loss, compare with the 3-layer MLP. What do you find?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: enter your discussion here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a saved model weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading a saved model weight from a file (like the one above) is also simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the best validation model for the MLP\n",
    "model_MLP.load_state_dict(torch.load('best_model_MLP.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging a Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debugging a neural network is not the same as debugging other codes, since there can be various reasons your model doesn't work or you have NaNs in your outputs. Here I'll cover several common issues and a few simple way to debug.\n",
    "\n",
    "The most straightforward way for debugging is to print some of the intermdeidate outputs or the network parameters. But you can also print the gradient of an intermediate output to see if it's in good range. To do this, you can either *register a hook* on that variable and ask it to print out its gradient, or directly call *.grad* to extract the gradient. Note that once you registered a hook, all receiving gradients will be printed out, but calling *.grad* will only print the gradient once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a simple computational graph\n",
    "a = nn.Parameter(torch.empty(10).uniform_(-1, 1))\n",
    "b = nn.Parameter(torch.empty(10).uniform_(-1, 1))\n",
    "c = (a * b).sum()\n",
    "# to print the gradient of a during backpropagation, register a 'print' hook on it\n",
    "a.register_hook(print)\n",
    "c.backward()\n",
    "# and let's check if that is the true gradient\n",
    "# the gradient received by a equals to b\n",
    "torch.allclose(b, a.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes you meet NaNs or inf in the gradient, and by printing it you may have a chance to find where are them. There could be many reasons for NaN/inf, and three most common ones are **gradient explosion**, **divide-by-zero**, and **gradient not defined**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient explosion\n",
    "a = nn.Parameter(torch.empty(10).uniform_(0, 1e2))\n",
    "b = (torch.pow(a, 10)).prod()\n",
    "# to print the gradient of a during backpropagation, register a 'print' hook on it\n",
    "a.register_hook(print)\n",
    "b.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here inf occurs when the gradient is too large. This often happens when you have power function. A simple way to avoid it is to use log when you apply power operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avoid gradient explosion by log\n",
    "log_a = torch.log(a)\n",
    "log_b = 50*log_a.sum()\n",
    "b = torch.exp(log_b)\n",
    "log_b.backward()  # use log_b to backpropagate instead of b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide-by-zero can occur in many places. I recommend you always add a small scalar (e.g. 1e-8) to the denominator whenever you have a division operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide-by-zero\n",
    "b = (a / 0).sum()\n",
    "b.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient not defined is a special case for some functions at some value. For example, if you manually calculate the power of L2-norm at 0, you will have NaN as gradient; but if you call *norm* function, you will have 0. This is related to the definition of **sub-gradient**, and I won't go to the details about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = nn.Parameter(torch.zeros(1))\n",
    "d.register_hook(print)\n",
    "e = d.pow(2).sqrt()\n",
    "e.backward()\n",
    "e = d.norm(p=2)\n",
    "e.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, I suggest you to always be careful to fractional exponents and check if a variable contains 0. There's actually a built-in function in Pytorch that helps you detect these issues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# automatically detect these issues\n",
    "from torch.autograd import detect_anomaly\n",
    "\n",
    "with detect_anomaly():\n",
    "    d = nn.Parameter(torch.zeros(1))\n",
    "    d.register_hook(print)\n",
    "    e = d.pow(2).sqrt()\n",
    "    e.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this will only detect NaN but not inf, and using *detect_anomaly()* will significantly slow down your training. Nevertheless, it is pretty useful when you try to find out what happened to your model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
